<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Tutorial &mdash; PyDGN 1.0.4 documentation</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="pydgn.data" href="source/pydgn.data.html" />
    <link rel="prev" title="Introduction" href="intro.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="index.html" class="icon icon-home"> PyDGN
            <img src="_static/pydgn-logo.svg" class="logo" alt="Logo"/>
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Intro and Usage</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#data-preprocessing">Data Preprocessing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-splitting">Data Splitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="#dataset-creation">Dataset Creation</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#experiment-setup">Experiment Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#data-information">Data Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#hardware">Hardware</a></li>
<li class="toctree-l3"><a class="reference internal" href="#data-loading">Data Loading</a></li>
<li class="toctree-l3"><a class="reference internal" href="#experiment-details">Experiment Details</a></li>
<li class="toctree-l3"><a class="reference internal" href="#grid-search">Grid Search</a></li>
<li class="toctree-l3"><a class="reference internal" href="#random-search">Random Search</a></li>
<li class="toctree-l3"><a class="reference internal" href="#experiment">Experiment</a></li>
<li class="toctree-l3"><a class="reference internal" href="#inspecting-results">Inspecting Results</a></li>
<li class="toctree-l3"><a class="reference internal" href="#profiling-information">Profiling Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="#tensorboard">Tensorboard</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Package Reference</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="source/pydgn.data.html">pydgn.data</a></li>
<li class="toctree-l1"><a class="reference internal" href="source/pydgn.evaluation.html">pydgn.evaluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="source/pydgn.experiment.html">pydgn.experiment</a></li>
<li class="toctree-l1"><a class="reference internal" href="source/pydgn.log.html">pydgn.log</a></li>
<li class="toctree-l1"><a class="reference internal" href="source/pydgn.model.html">pydgn.model</a></li>
<li class="toctree-l1"><a class="reference internal" href="source/pydgn.training.html">pydgn.training</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PyDGN</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
      <li>Tutorial</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/tutorial.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline"></a></h1>
<p>Knowing how to set up valid YAML configuration files is fundamental to properly use <strong>PyDGN</strong>. Custom behavior with
more advanced functionalities can be generally achieved by subclassing the individual modules we provide,
but this is very much dependent on the specific research project.</p>
<section id="data-preprocessing">
<h2>Data Preprocessing<a class="headerlink" href="#data-preprocessing" title="Permalink to this headline"></a></h2>
<p>The ML pipeline starts with the creation of the dataset and of the data splits. The general template that we can use is
the following, with an explanation of each field as a comment:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">splitter</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">root</span><span class="p">:</span><span class="w">  </span><span class="c1"># folder where to store the splits</span><span class="w"></span>
<span class="w">  </span><span class="nt">class_name</span><span class="p">:</span><span class="w">  </span><span class="c1"># dotted path to splitter class</span><span class="w"></span>
<span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">n_outer_folds</span><span class="p">:</span><span class="w">  </span><span class="c1"># number of outer folds for risk assessment</span><span class="w"></span>
<span class="w">    </span><span class="nt">n_inner_folds</span><span class="p">:</span><span class="w">  </span><span class="c1"># number of inner folds for model selection</span><span class="w"></span>
<span class="w">    </span><span class="nt">seed</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">stratify</span><span class="p">:</span><span class="w">  </span><span class="c1"># target stratification: works for graph classification tasks only</span><span class="w"></span>
<span class="w">    </span><span class="nt">shuffle</span><span class="p">:</span><span class="w">  </span><span class="c1"># whether to shuffle the indices prior to splitting</span><span class="w"></span>
<span class="w">    </span><span class="nt">inner_val_ratio</span><span class="p">:</span><span class="w">  </span><span class="c1"># percentage of validation for hold-out model selection. this will be ignored when the number of inner folds is &gt; than 1</span><span class="w"></span>
<span class="w">    </span><span class="nt">outer_val_ratio</span><span class="p">:</span><span class="w">  </span><span class="c1"># percentage of validation data to extract for risk assessment final runs</span><span class="w"></span>
<span class="w">    </span><span class="nt">test_ratio</span><span class="p">:</span><span class="w">  </span><span class="c1"># percentage of test to extract for hold-out risk assessment. this will be ignored when the number of outer folds is &gt; than 1</span><span class="w"></span>
<span class="nt">dataset</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">root</span><span class="p">:</span><span class="w">  </span><span class="c1"># path to data root folder</span><span class="w"></span>
<span class="w">  </span><span class="nt">class_name</span><span class="p">:</span><span class="w">  </span><span class="c1"># dotted path to dataset class</span><span class="w"></span>
<span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w">  </span><span class="c1"># arguments to pass to the dataset class</span><span class="w"></span>
<span class="w">    </span><span class="nt">arg_name1</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">arg_namen</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">transform</span><span class="p">:</span><span class="w"> </span><span class="c1"># on the fly transforms: useful for social datasets with no node features (with an example)</span><span class="w"></span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.data.transform.ConstantIfEmpty</span><span class="w"></span>
<span class="w">      </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">        </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">  </span><span class="c1"># pre_transform:  # transform data and store it at dataset creation time</span><span class="w"></span>
<span class="w">  </span><span class="c1"># pre_filter:  # filter data and store it at dataset creation time</span><span class="w"></span>
</pre></div>
</div>
<section id="data-splitting">
<h3>Data Splitting<a class="headerlink" href="#data-splitting" title="Permalink to this headline"></a></h3>
<p>We provide a general <a class="reference internal" href="source/pydgn.data.html#pydgn.data.splitter.Splitter" title="pydgn.data.splitter.Splitter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Splitter</span></code></a> class that is able to split a dataset of multiple graphs. The most important parameters
are arguably <code class="docutils literal notranslate"><span class="pre">n_outer_folds</span></code> and <code class="docutils literal notranslate"><span class="pre">n_inner_folds</span></code>, which represent the way in which we want to perform <strong>risk assessment</strong>
and <strong>model selection</strong>. For instance:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">n_outer_folds=10</span></code> and <code class="docutils literal notranslate"><span class="pre">n_inner_folds=1</span></code>: 10-fold external Cross Validation (CV) on test data, with hold-out model selection inside each of the 10 folds,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_outer_folds=5</span></code> and <code class="docutils literal notranslate"><span class="pre">n_inner_folds=3</span></code>: Nested CV,</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">n_outer_folds=1</span></code> and <code class="docutils literal notranslate"><span class="pre">n_inner_folds=1</span></code>: Simple Hold-out model assessment and selection, or <code class="docutils literal notranslate"><span class="pre">train/val/test</span></code> split.</p></li>
</ul>
</div></blockquote>
<p>We assume that the difference between <strong>risk assessment</strong> and <strong>model selection</strong> is clear to the reader.
If not, please refer to <a class="reference external" href="https://bengio.abracadoudou.com/lectures/theory.pdf">Samy Bengio’s lecture (Part 3)</a>.</p>
<p>Here’s an snippet of a potential configuration file that splits a graph classification dataset:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">splitter</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">root</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">examples/DATA_SPLITS/CHEMICAL</span><span class="w"></span>
<span class="w">  </span><span class="nt">class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.data.splitter.Splitter</span><span class="w"></span>
<span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">n_outer_folds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span><span class="w"></span>
<span class="w">    </span><span class="nt">n_inner_folds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span><span class="w"></span>
<span class="w">    </span><span class="nt">seed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">42</span><span class="w"></span>
<span class="w">    </span><span class="nt">stratify</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"></span>
<span class="w">    </span><span class="nt">shuffle</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"></span>
<span class="w">    </span><span class="nt">inner_val_ratio</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class="w"></span>
<span class="w">    </span><span class="nt">outer_val_ratio</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class="w"></span>
<span class="w">    </span><span class="nt">test_ratio</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="dataset-creation">
<h3>Dataset Creation<a class="headerlink" href="#dataset-creation" title="Permalink to this headline"></a></h3>
<p>To create your own dataset, you should implement the <a class="reference internal" href="source/pydgn.data.html#pydgn.data.dataset.DatasetInterface" title="pydgn.data.dataset.DatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetInterface</span></code></a> interface. For
instance, we provide a wrapper around the <a class="reference external" href="https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.TUDataset">TUDataset</a>
dataset of Pytorch Geometric in <a class="reference internal" href="source/pydgn.data.html#pydgn.data.dataset.TUDatasetInterface" title="pydgn.data.dataset.TUDatasetInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">TUDatasetInterface</span></code></a>, which you can check to get an idea.</p>
<p>Here’s an snippet of a potential configuration file that downloads and processes the <code class="docutils literal notranslate"><span class="pre">PROTEINS</span></code> graph classification dataset:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">dataset</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">root</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DATA/</span><span class="w"></span>
<span class="w">  </span><span class="nt">class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.data.dataset.TUDataset</span><span class="w"></span>
<span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">root</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DATA/</span><span class="w"></span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">PROTEINS</span><span class="w"></span>
</pre></div>
</div>
<p>You can also apply <code class="docutils literal notranslate"><span class="pre">transform</span></code>, <code class="docutils literal notranslate"><span class="pre">pre_transform</span></code> and <code class="docutils literal notranslate"><span class="pre">pre_filter</span></code> that follow the same semantic of PyG.</p>
<p>Once our data configuration file is ready, we can create the dataset using (for the example above)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pydgn-dataset --config-file examples/DATA_CONFIGS/config_PROTEINS.yml
</pre></div>
</div>
</section>
</section>
<section id="experiment-setup">
<h2>Experiment Setup<a class="headerlink" href="#experiment-setup" title="Permalink to this headline"></a></h2>
<p>Once we have created a dataset and its data splits, it is time to implement our model and define a suitable task.
Every model must implement the <a class="reference internal" href="source/pydgn.model.html#pydgn.model.interface.ModelInterface" title="pydgn.model.interface.ModelInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">ModelInterface</span></code></a> interface, and it can optionally use a
readout module that must implement the <a class="reference internal" href="source/pydgn.model.html#pydgn.model.interface.ReadoutInterface" title="pydgn.model.interface.ReadoutInterface"><code class="xref py py-class docutils literal notranslate"><span class="pre">ReadoutInterface</span></code></a>.</p>
<p>At this point, it is time to define the experiment. The general template that we can use is the following, with an
explanation of each field as a comment:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Dataset and Splits</span><span class="w"></span>
<span class="nt">data_root</span><span class="p">:</span><span class="w">  </span><span class="c1"># path to DATA root folder (same as in data config file)</span><span class="w"></span>
<span class="nt">dataset_class</span><span class="p">:</span><span class="w">  </span><span class="c1"># dotted path to dataset class</span><span class="w"></span>
<span class="nt">dataset_name</span><span class="p">:</span><span class="w">  </span><span class="c1"># dataset name (same as in data config file)</span><span class="w"></span>
<span class="nt">data_splits_file</span><span class="p">:</span><span class="w">  </span><span class="c1"># path to data splits file</span><span class="w"></span>


<span class="c1"># Hardware</span><span class="w"></span>
<span class="nt">device</span><span class="p">:</span><span class="w">  </span><span class="c1"># cpu | cuda</span><span class="w"></span>
<span class="nt">max_cpus</span><span class="p">:</span><span class="w">  </span><span class="c1"># &gt; 1 for parallelism</span><span class="w"></span>
<span class="nt">max_gpus</span><span class="p">:</span><span class="w"> </span><span class="c1"># &gt; 0 for gpu usage (device must be cuda though)</span><span class="w"></span>
<span class="nt">gpus_per_task</span><span class="p">:</span><span class="w">  </span><span class="c1"># percentage of gpus to allocate for each task</span><span class="w"></span>


<span class="c1"># Data Loading</span><span class="w"></span>
<span class="nt">dataset_getter</span><span class="p">:</span><span class="w">  </span><span class="c1"># dotted path to dataset provider class</span><span class="w"></span>
<span class="nt">data_loader</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">class_name</span><span class="p">:</span><span class="w">  </span><span class="c1"># dotted path to data loader class</span><span class="w"></span>
<span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">num_workers </span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">pin_memory</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="c1"># possibly other arguments (we set `worker_init_fn`, `sampler` and `shuffle`, so do not override)</span><span class="w"></span>


<span class="c1"># Reproducibility</span><span class="w"></span>
<span class="nt">seed</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">42</span><span class="w"></span>


<span class="c1"># Experiment</span><span class="w"></span>
<span class="nt">result_folder</span><span class="p">:</span><span class="w">  </span><span class="c1"># path of the folder where to store results</span><span class="w"></span>
<span class="nt">exp_name</span><span class="p">:</span><span class="w">  </span><span class="c1"># name of the experiment</span><span class="w"></span>
<span class="nt">experiment</span><span class="p">:</span><span class="w">  </span><span class="c1"># dotted path to experiment class</span><span class="w"></span>
<span class="nt">higher_results_are_better</span><span class="p">:</span><span class="w">  </span><span class="c1"># model selection: should we select based on max (True) or min (False) main score?</span><span class="w"></span>
<span class="nt">evaluate_every</span><span class="p">:</span><span class="w">  </span><span class="c1"># evaluate on train/val/test every `n` epochs and log results</span><span class="w"></span>
<span class="nt">final_training_runs</span><span class="p">:</span><span class="w">  </span><span class="c1"># how many final (model assessment) training runs to perform to mitigate bad initializations</span><span class="w"></span>

<span class="c1"># Grid Search</span><span class="w"></span>
<span class="c1"># if only 1 configuration is selected, any inner model selection will be skipped</span><span class="w"></span>
<span class="nt">grid</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">supervised_config</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">model</span><span class="p">:</span><span class="w">  </span><span class="c1"># dotted path to model class</span><span class="w"></span>
<span class="w">    </span><span class="nt">checkpoint</span><span class="p">:</span><span class="w">  </span><span class="c1"># whether to keep a checkpoint of the last epoch to resume training</span><span class="w"></span>
<span class="w">    </span><span class="nt">shuffle</span><span class="p">:</span><span class="w">  </span><span class="c1"># whether to shuffle the data</span><span class="w"></span>
<span class="w">    </span><span class="nt">batch_size</span><span class="p">:</span><span class="w">  </span><span class="c1"># batch size</span><span class="w"></span>
<span class="w">    </span><span class="nt">epochs</span><span class="p">:</span><span class="w">  </span><span class="c1"># number of maximum training epochs</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Model specific arguments #</span><span class="w"></span>

<span class="w">    </span><span class="c1"># TBD</span><span class="w"></span>

<span class="w">    </span><span class="c1"># ------------------------ #</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Optimizer (with an example - 3 possible alternatives)</span><span class="w"></span>
<span class="w">    </span><span class="nt">optimizer</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.optimizer.Optimizer</span><span class="w"></span>
<span class="w">        </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">optimizer_class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch.optim.Adam</span><span class="w"></span>
<span class="w">          </span><span class="nt">lr</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.01</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.001</span><span class="w"></span>
<span class="w">          </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.optimizer.Optimizer</span><span class="w"></span>
<span class="w">        </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">optimizer_class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch.optim.Adagrad</span><span class="w"></span>
<span class="w">          </span><span class="nt">lr</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.1</span><span class="w"></span>
<span class="w">          </span><span class="nt">weight_decay</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0.</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Scheduler (optional)</span><span class="w"></span>
<span class="w">    </span><span class="nt">scheduler</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Loss metric (with an example of Additive Loss)</span><span class="w"></span>
<span class="w">    </span><span class="nt">loss</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.metric.AdditiveLoss</span><span class="w"></span>
<span class="w">        </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">loss_1</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.metric.MulticlassClassification</span><span class="w"></span>
<span class="w">          </span><span class="nt">loss_2</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.metric.MulticlassClassification</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Score metric (with an example of Multi Score)</span><span class="w"></span>
<span class="w">    </span><span class="nt">scorer</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.metric.MultiScore</span><span class="w"></span>
<span class="w">        </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">main_scorer</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.metric.MulticlassAccuracy</span><span class="w"></span>
<span class="w">          </span><span class="nt">my_second_metric</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.metric.ToyMetric</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Readout (optional)</span><span class="w"></span>
<span class="w">    </span><span class="nt">readout</span><span class="p">:</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Training engine</span><span class="w"></span>
<span class="w">    </span><span class="nt">engine</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.engine.TrainingEngine</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Gradient clipper (optional)</span><span class="w"></span>
<span class="w">    </span><span class="nt">gradient_clipper</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">null</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Early stopper (optional, with an example of &quot;patience&quot; early stopping on the validation score)</span><span class="w"></span>
<span class="w">    </span><span class="nt">early_stopper</span><span class="p">:</span><span class="w"></span>
<span class="w">      </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">class_name</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.early_stopping.PatienceEarlyStopper</span><span class="w"></span>
<span class="w">        </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">          </span><span class="nt">patience</span><span class="p">:</span><span class="w"></span>
<span class="w">            </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span><span class="w"></span>
<span class="w">          </span><span class="c1"># SYNTAX: (train_,validation_)[name_of_the_scorer_or_loss_to_monitor] -&gt; we can use MAIN_LOSS or MAIN_SCORE</span><span class="w"></span>
<span class="w">          </span><span class="nt">monitor</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">validation_main_score</span><span class="w"></span>
<span class="w">          </span><span class="nt">mode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">max</span><span class="w">  </span><span class="c1"># is best the `max` or the `min` value we are monitoring?</span><span class="w"></span>
<span class="w">          </span><span class="nt">checkpoint</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w">  </span><span class="c1"># store the best checkpoint</span><span class="w"></span>

<span class="w">    </span><span class="c1"># Plotter of metrics</span><span class="w"></span>
<span class="w">    </span><span class="nt">plotter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.training.callback.plotter.Plotter</span><span class="w"></span>
</pre></div>
</div>
<section id="data-information">
<h3>Data Information<a class="headerlink" href="#data-information" title="Permalink to this headline"></a></h3>
<p>Here we can specify some information about the dataset:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">data_root</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">DATA</span><span class="w"></span>
<span class="nt">dataset_class</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.data.dataset.TUDatasetInterface</span><span class="w"></span>
<span class="nt">dataset_name</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">PROTEINS</span><span class="w"></span>
<span class="nt">data_splits_file</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">examples/DATA_SPLITS/CHEMICAL/PROTEINS/PROTEINS_outer10_inner1.splits</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="hardware">
<h3>Hardware<a class="headerlink" href="#hardware" title="Permalink to this headline"></a></h3>
<p>Here we can define how many resources to allocate to parallelize different experiments:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># this will run a maximum of 4 experiments to allocate all of the 2 gpus we have.</span><span class="w"></span>
<span class="c1"># We use some more cpu resources to take into account potential `data loader workers &lt;https://pytorch.org/docs/stable/data.html#multi-process-data-loading&gt;`_.</span><span class="w"></span>
<span class="nt">device</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">cuda</span><span class="w"></span>
<span class="nt">max_cpus</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">8</span><span class="w"></span>
<span class="nt">max_gpus</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"></span>
<span class="nt">gpus_per_task</span><span class="p">:</span><span class="w">  </span><span class="l l-Scalar l-Scalar-Plain">0.5</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="data-loading">
<h3>Data Loading<a class="headerlink" href="#data-loading" title="Permalink to this headline"></a></h3>
<p>Here we specify which <a class="reference internal" href="source/pydgn.data.html#pydgn.data.provider.DataProvider" title="pydgn.data.provider.DataProvider"><code class="xref py py-class docutils literal notranslate"><span class="pre">DataProvider</span></code></a> we want to use to load the data associated with the
given splits, and the <code class="xref py py-class docutils literal notranslate"><span class="pre">DataLoader</span></code> that needs to handle such data:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="c1"># Data Loading</span><span class="w"></span>
<span class="nt">dataset_getter</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.data.provider.DataProvider</span><span class="w"></span>
<span class="nt">data_loader</span><span class="p">:</span><span class="w"></span>
<span class="w">  </span><span class="nt">class_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">torch_geometric.loader.DataLoader</span><span class="w"></span>
<span class="w">  </span><span class="nt">args</span><span class="p">:</span><span class="w"></span>
<span class="w">    </span><span class="nt">num_workers </span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span><span class="w"></span>
<span class="w">    </span><span class="nt">pin_memory</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w">  </span><span class="c1"># should be True when device is set to `cuda`</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="experiment-details">
<h3>Experiment Details<a class="headerlink" href="#experiment-details" title="Permalink to this headline"></a></h3>
<p>Here we define the experiment details, including the experiment name and type, and the folder where we want to store
our results:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">result_folder</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">RESULTS</span><span class="w"></span>
<span class="nt">exp_name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">supervised_grid_search_toy</span><span class="w"></span>
<span class="nt">experiment</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">pydgn.experiment.supervised_task.SupervisedTask</span><span class="w"></span>
<span class="nt">higher_results_are_better</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">True</span><span class="w"></span>
<span class="nt">evaluate_every</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class="w"></span>
<span class="nt">final_training_runs</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span><span class="w"></span>
</pre></div>
</div>
</section>
<section id="grid-search">
<h3>Grid Search<a class="headerlink" href="#grid-search" title="Permalink to this headline"></a></h3>
<p>Grid search is identified by the keyword <code class="docutils literal notranslate"><span class="pre">grid</span></code> after the experimental details. This is the easiest setting, in which
you can define lists associated to an hyper-parameter and all possible combinations will be created. You can even have
nesting of these combinations for maximum flexibility.</p>
<p>There is one config file <code class="docutils literal notranslate"><span class="pre">examples/MODEL_CONFIGS/config_SupToyDGN.yml</span></code> that you can check to get a better idea.</p>
</section>
<section id="random-search">
<h3>Random Search<a class="headerlink" href="#random-search" title="Permalink to this headline"></a></h3>
<p>Random search, on the other hand, is identified by the keyword <code class="docutils literal notranslate"><span class="pre">random</span></code> after the experimental details. One line above
we have to specify the number of random trials, using the keyword <code class="docutils literal notranslate"><span class="pre">num_samples</span></code>.</p>
<dl class="simple">
<dt>We provide different sampling methods:</dt><dd><ul class="simple">
<li><p>choice –&gt; pick at random from a list of arguments</p></li>
<li><p>uniform –&gt; pick uniformly from min and max arguments</p></li>
<li><p>normal –&gt; sample from normal distribution with mean and std</p></li>
<li><p>randint –&gt; pick at random from min and max</p></li>
<li><p>loguniform –&gt; pick following the recprocal distribution from log_min, log_max, with a specified base</p></li>
</ul>
</dd>
</dl>
<p>There is one config file <code class="docutils literal notranslate"><span class="pre">examples/MODEL_CONFIGS/config_SupToyDGN_RandomSearch.yml</span></code> that you can check to get a better idea.</p>
</section>
<section id="experiment">
<h3>Experiment<a class="headerlink" href="#experiment" title="Permalink to this headline"></a></h3>
<p>Depending on the experiment type, different main keywords are required:</p>
<blockquote>
<div><ul class="simple">
<li><p><a class="reference internal" href="source/pydgn.experiment.html#pydgn.experiment.supervised_task.SupervisedTask" title="pydgn.experiment.supervised_task.SupervisedTask"><code class="xref py py-class docutils literal notranslate"><span class="pre">SupervisedTask</span></code></a> -&gt; <code class="docutils literal notranslate"><span class="pre">supervised_config</span></code></p></li>
<li><p><a class="reference internal" href="source/pydgn.experiment.html#pydgn.experiment.semi_supervised_task.SemiSupervisedTask" title="pydgn.experiment.semi_supervised_task.SemiSupervisedTask"><code class="xref py py-class docutils literal notranslate"><span class="pre">SemiSupervisedTask</span></code></a> -&gt; <code class="docutils literal notranslate"><span class="pre">unsupervised_config</span></code> and <code class="docutils literal notranslate"><span class="pre">supervised_config</span></code></p></li>
</ul>
</div></blockquote>
<p>Note that an <strong>unsupervised</strong> task may be seen as a supervised task with a loss objective that does not depend on the target.
In the <strong>supervised</strong> task, we have the “standard” training procedure. In the <strong>semi-supervised</strong> task, we expect a first
training phase which is unsupervised and that can produce node/graph embeddings. After that, a second, supervised model
is trained on such embeddings, but adjacency information is not preserved in this second stage.</p>
<p>Inside the dictionary associated to the keyword <code class="docutils literal notranslate"><span class="pre">[un]supervised_config</span></code>, we expect another number of keywords to be
present:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">model</span><span class="p">:</span><span class="w">  </span><span class="c1"># dotted path to model class</span><span class="w"></span>
<span class="nt">checkpoint</span><span class="p">:</span><span class="w">  </span><span class="c1"># whether to keep a checkpoint of the last epoch to resume training</span><span class="w"></span>
<span class="nt">shuffle</span><span class="p">:</span><span class="w">  </span><span class="c1"># whether to shuffle the data</span><span class="w"></span>
<span class="nt">batch_size</span><span class="p">:</span><span class="w"></span>
<span class="nt">epochs</span><span class="p">:</span><span class="w">  </span><span class="c1"># number of maximum training epochs</span><span class="w"></span>

<span class="c1"># Model specific arguments #</span><span class="w"></span>

<span class="c1"># ------------------------ #</span><span class="w"></span>

<span class="nt">optimizer</span><span class="p">:</span><span class="w"></span>
<span class="nt">scheduler</span><span class="p">:</span><span class="w">  </span><span class="c1"># (can be &quot;null&quot;)</span><span class="w"></span>
<span class="nt">loss</span><span class="p">:</span><span class="w"></span>
<span class="nt">scorer</span><span class="p">:</span><span class="w"></span>
<span class="nt">readout</span><span class="p">:</span><span class="w">   </span><span class="c1"># (can be &quot;null&quot;)</span><span class="w"></span>
<span class="nt">engine</span><span class="p">:</span><span class="w">  </span><span class="c1"># training engine</span><span class="w"></span>
<span class="nt">gradient_clipper</span><span class="p">:</span><span class="w">  </span><span class="c1"># (can be &quot;null&quot;)</span><span class="w"></span>
<span class="nt">early_stopper</span><span class="p">:</span><span class="w">   </span><span class="c1"># (can be &quot;null&quot;)</span><span class="w"></span>
<span class="nt">plotter</span><span class="p">:</span><span class="w">   </span><span class="c1"># (can be &quot;null&quot;)</span><span class="w"></span>
</pre></div>
</div>
<p>Once our experiment configuration file is ready, we can launch an experiment using (see below for a couple of examples)</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pydgn-dataset --config-file examples/MODEL_CONFIGS/config_SupToyDGN.yml
</pre></div>
</div>
<p>or</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>pydgn-dataset --config-file examples/MODEL_CONFIGS/config_SemiSupToyDGN.yml
</pre></div>
</div>
<p>And we are up and running!</p>
<a class="reference internal image-reference" href="_images/exp_gui.png"><img alt="_images/exp_gui.png" src="_images/exp_gui.png" style="width: 600px;" /></a>
<p>To stop the computation, use <code class="docutils literal notranslate"><span class="pre">CTRL-C</span></code> to send a <code class="docutils literal notranslate"><span class="pre">SIGINT</span></code> signal, and consider using the command <code class="docutils literal notranslate"><span class="pre">ray</span> <span class="pre">stop</span></code> to stop
all Ray processes. <strong>Warning:</strong> <code class="docutils literal notranslate"><span class="pre">ray</span> <span class="pre">stop</span></code> stops <strong>all</strong> ray processes you have launched, including those of other
experiments in progress, if any.</p>
<p>Some things to notice: because we have chosen a 10-fold CV for risk assessment with a simple hold-out model selection <strong>for
each</strong> external fold, you can notice in the picture there are <code class="docutils literal notranslate"><span class="pre">10*1</span></code> rows with <code class="docutils literal notranslate"><span class="pre">Out_*/Inn_*</span></code> written. For each of these,
we have to perform a model selection with <code class="docutils literal notranslate"><span class="pre">4</span></code> possible hyper-parameters’ configurations (progress shown on the right handside).
In addition, there are also some stats about the time required to complete the experiments.</p>
<p>After the 10 model selection are complete (i.e., one “best” model for each outer/external fold), it is time to re-train
the chosen models on the 10 different train/test splits. Therefore, you can notice <code class="docutils literal notranslate"><span class="pre">10</span></code> rows with <code class="docutils literal notranslate"><span class="pre">Final</span> <span class="pre">run</span> <span class="pre">*</span></code> written.
Since we have specified <code class="docutils literal notranslate"><span class="pre">final_training_runs:</span> <span class="pre">3</span></code> in our exp. config file, we will mitigate unlucky random initializations
of the chosen models by averaging test results (of a single outer fold) over 3 training runs. The final generalization
performances of the model (a less ambiguous definition would be: the <strong>class of models</strong> you developed) is obtained,
for this specific case, as the average of the 10 test scores across the external folds. Again, if this does not make sense
to you, please consider reading <a class="reference external" href="https://bengio.abracadoudou.com/lectures/theory.pdf">Samy Bengio’s lecture (Part 3)</a>.</p>
</section>
<section id="inspecting-results">
<h3>Inspecting Results<a class="headerlink" href="#inspecting-results" title="Permalink to this headline"></a></h3>
<p>According to our configuration file, the results are stored in the <code class="docutils literal notranslate"><span class="pre">RESULTS</span></code> folder. The hierarchy of folder is the following:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">supervised_grid_search_toy_PROTEINS</span>
<span class="o">|</span><span class="n">__</span> <span class="n">MODEL_ASSESSMENT</span>
    <span class="o">|</span><span class="n">__</span> <span class="n">assessment_results</span><span class="o">.</span><span class="n">json</span>  <span class="c1"># contains the risk assessment results (average of the outer fold results)</span>
    <span class="o">|</span><span class="n">__</span> <span class="n">OUTER_FOLD_1</span>
    <span class="o">...</span>
    <span class="o">|</span><span class="n">__</span> <span class="n">OUTER_FOLD_5</span>
        <span class="o">|</span><span class="n">__</span> <span class="n">outer_results</span><span class="o">.</span><span class="n">json</span>  <span class="c1"># contains the aggregated results of the three final runs</span>
        <span class="o">|</span><span class="n">__</span> <span class="n">final_run_1</span>
        <span class="o">|</span><span class="n">__</span> <span class="n">final_run_2</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">tensorboard</span>  <span class="c1"># tensorboard folder</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">experiment</span><span class="o">.</span><span class="n">log</span>  <span class="c1"># log file with profiling information</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">best_checkpoint</span><span class="o">.</span><span class="n">pth</span>  <span class="c1"># torch dict holding the &quot;best&quot; checkpoint information according to the early stopper used</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">last_checkpoint</span><span class="o">.</span><span class="n">pth</span>  <span class="c1"># torch dict holding the checkpoint information of the last epoch (top ``checkpoint`` keyword set to true)</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">run_2_results</span><span class="o">.</span><span class="n">torch</span>  <span class="c1"># torch dict holding the results of the 2nd final run on the 5th outer fold.</span>
        <span class="o">|</span><span class="n">__</span> <span class="n">final_run_3</span>
        <span class="o">|</span><span class="n">__</span> <span class="n">MODEL_SELECTION</span>  <span class="c1"># files regarding the model selection inside the 5th outer fold</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">winner_config</span><span class="o">.</span><span class="n">json</span>  <span class="c1"># contains the &quot;best model&quot; across the inner folds (in this case just 1 inner fold) for the 5th fold to be used in the final training runs</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">config_1</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">config_2</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">config_3</span>
                <span class="o">|</span><span class="n">__</span> <span class="n">config_results</span><span class="o">.</span><span class="n">json</span>  <span class="c1"># contains the aggregated results of the K inner model selection folds</span>
                <span class="o">|</span><span class="n">__</span> <span class="n">INNER_FOLD_1</span>  <span class="c1"># first (and only in this case) inner model selection fold</span>
                    <span class="o">|</span><span class="n">__</span> <span class="n">tensorboard</span>
                    <span class="o">|</span><span class="n">__</span> <span class="n">experiment</span><span class="o">.</span><span class="n">log</span>  <span class="c1"># log file with profiling information</span>
                    <span class="o">|</span><span class="n">__</span> <span class="n">best_checkpoint</span><span class="o">.</span><span class="n">pth</span>
                    <span class="o">|</span><span class="n">__</span> <span class="n">last_checkpoint</span><span class="o">.</span><span class="n">pth</span>
                    <span class="o">|</span><span class="n">__</span> <span class="n">fold_1_results</span><span class="o">.</span><span class="n">torch</span>  <span class="c1"># torch dict holding the results of the 1st fold results of the 3rd configuration.</span>
            <span class="o">|</span><span class="n">__</span> <span class="n">config_4</span>
    <span class="o">...</span>
    <span class="o">|</span><span class="n">__</span> <span class="n">OUTER_FOLD_10</span>
</pre></div>
</div>
</section>
<section id="profiling-information">
<h3>Profiling Information<a class="headerlink" href="#profiling-information" title="Permalink to this headline"></a></h3>
<p>Inside each <code class="docutils literal notranslate"><span class="pre">experiment.log</span></code> file, you will find training logs and, at the end of each training, the profiler information
with the per-epoch and total time required by each <a class="reference internal" href="source/pydgn.training.event.html#pydgn.training.event.handler.EventHandler" title="pydgn.training.event.handler.EventHandler"><code class="xref py py-class docutils literal notranslate"><span class="pre">EventHandler</span></code></a>, provided the
time spent is non-negligible (threshold specified in the log file).</p>
<p>Here’s what it looks like:</p>
<a class="reference internal image-reference" href="_images/profiler.png"><img alt="_images/profiler.png" src="_images/profiler.png" style="width: 600px;" /></a>
</section>
<section id="tensorboard">
<h3>Tensorboard<a class="headerlink" href="#tensorboard" title="Permalink to this headline"></a></h3>
<p>We can use the generic <a class="reference internal" href="source/pydgn.training.callback.html#pydgn.training.callback.plotter.Plotter" title="pydgn.training.callback.plotter.Plotter"><code class="xref py py-class docutils literal notranslate"><span class="pre">Plotter</span></code></a> class to easily visualize the training trend with
Tensorboard, using the information in the <code class="docutils literal notranslate"><span class="pre">tensorboard</span></code> folder:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>tensorboard --logdir RESULTS/supervised_grid_search_toy_PROTEINS/MODEL_ASSESSMENT/OUTER_FOLD_1/final_run1/tensorboard/
</pre></div>
</div>
<p>And we get:</p>
<a class="reference internal image-reference" href="_images/tensorboard.png"><img alt="_images/tensorboard.png" src="_images/tensorboard.png" style="width: 600px;" /></a>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="intro.html" class="btn btn-neutral float-left" title="Introduction" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="source/pydgn.data.html" class="btn btn-neutral float-right" title="pydgn.data" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2022, Federico Errica.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>